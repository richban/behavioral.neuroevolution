\chapter{Experimental Design}

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

\section{Test Bed}
The experimental test bed set up was inspired by Jacobsen and Fai√±a \citep{faina2017automating}, which consists of a physical arena, in which the evolutionary optimization of the \emph{Thymio} robot is performed and a computer vision system. The computer vision system is used to build the virtual environment in simulation based on the environmental objects e.g. \emph{obstacles} and to track the \emph{Thymio} robot during the evaluation. The main components of the test bed are shown in the following figure \ref{fig:test_bed}.

\begin{figure}[H]
  \centering
  \includegraphics[width=6cm]{include/images/test_bed.PNG}
  \caption{The test bed, composed of computer vision, physical arena containing the environmental objects and the Thymio robot and lastly the virtual simulation.}
  \label{fig:test_bed}
\end{figure}

\subsection{Physical Arena and Virtual Arena}

The outlook of the experiments is to validate different reality gap approaches on an obstacle avoidance task. This required to build a \emph{physical arena} where the entire reality-based optimization is taking place. The physical arena is relatively small, measures only 119 x 80 x 20 cm, however, given the simplicity of the task it's sufficient enough to perform our automated experiments. It's assembled of 4 pieces of wooden planks tight together on each side to prevent the robot to push them outside their boundary. Each of the corners is marked with a fiducial marker to assist the computer vision and the evolutionary process. The arena is divided into 3 different sectors, namely, \emph{area0, area1, area2}, that is used to measure certain behavioral features during the evolution. In order to perform simulation-based and robot-in-the-loop optimization process, a simulation model (\emph{virtual arena}) was created accordingly to the \emph{physical arena}. The physical arena can be seen in figure \ref{fig:physical_arena} and it's virtual correspondent in figure \ref{fig:virtual_arena}.


\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
    	\centering
        \includegraphics[width=6cm]{include/images/physical_arena.PNG}
        \caption{Physical Arena.}
        \label{fig:physical_arena}
    \end{subfigure}
    \begin{subfigure}[b]{0.4\textwidth}
    	\centering
        \includegraphics[width=6cm]{include/images/virtual_arena.PNG}
        \caption{Virtual Arena.}
        \label{fig:virtual_arena}
    \end{subfigure}
    \caption{Physical and virtual environment.}\label{sistemass1}
\end{figure}

\subsection{Obstacles}

Each arena (\emph{physical arena} and \emph{virtual arena}), contains 3 different cuboids in sizes. These are the only environmental objects that the robots interact with. Obstacles in the \emph{physical arena} are heavy enough, therefore they are not knocked over by \tmph{Thymio} robot during a collision. Additionally, each obstacle is marked with a fiducial marker, in order to set the position of its counterpart \emph{virtual obstacle}\ref{fig:virtual_obstacle} in the \emph{virtual arena}.

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
    	\centering
        \includegraphics[width=6cm]{include/images/obstacle_physical.PNG}
        \caption{Physical obstacle.}
        \label{fig:physical_obstacle}
    \end{subfigure}
    \begin{subfigure}[b]{0.4\textwidth}
    	\centering
        \includegraphics[width=6cm]{include/images/obstacle_virtual.PNG}
        \caption{Virtual obstacle.}
        \label{fig:virtual_obstacle}
    \end{subfigure}
    \caption{Environmental objects.}\label{sistemass1}
\end{figure}

\section{Robots}

One of the key component of our experiments is the physical robot. The main role that the robot system plays in our experimental setting is a black box to be programmed in order to create a concrete physical manifestation of the evolved robot control policies. In this case, the robot is not built, but rather a well functioning preassembled robot that serves as a supporting component. For our purposes, we focused on preexisting robotic platform, which comes with robot controller and programming environment, namely \emph{Thymio}.

\subsection{Thymio}

\emph{Thymio} \citep{mondada2017thymio} is a small robot produced by Mobsya\footnote{\url{http://www.mobsya.org/}} for educational purposes. It has 5 proximity sensors in front, 2 proximity sensors on the back as well as 2 grounds sensors. Thymio is also equipped with a temperature sensor, various buttons for interaction, visual sensors etc. A detailed overview of all the components can be seen in figure \ref{fig:thymio}. For this experiments purposes, we use the new \emph{Wireless Thymio}, which enables us to control the robot with a wireless dongle.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
    	\centering
        \includegraphics[width=6cm]{include/images/thymio.PNG}
        \caption{Components of Thymio.}
        \label{fig:thymio}
    \end{subfigure}
    \begin{subfigure}[b]{0.4\textwidth}
    	\centering
        \includegraphics[width=6cm]{include/images/thymio_1.PNG}
        \caption{Thymio II model with attached powerbank.}
        \label{fig:real_thymio}
    \end{subfigure}
    % \caption{Environmental objects.}\label{sistemass1}
\end{figure}

Thymio robot supports autonomous charging, between 4 and 5 hours of autonomy. In order to evaluate a single evaluation run without any intervetion, we placed a powerbank on top of the thymio. The main characteristics of the Thymio robot is summed up in this table:

\begin{table}[ht]
\centering
\begin{tabular}{|c|L{0.8\textwidth}|}
\hline
Characteristics & \multicolumn{1}{c|}{Values} \\ \hline
\texttt{Length} & 112 mm \\
\texttt{Width} & 117 mm \\
\texttt{Height} & 53 mm \\
\texttt{Weight} & 250 g \\
\texttt{Max. speed} & 0.2 m/s \\
\texttt{IR Sensors} & front.prox.horizontal.[0-4], back.prox.horizontal.[5-6], prox.ground.[0-1]; Proximity sensor response values between range 0-4500;  \\
\texttt{Battery} & Li-Po Battery: 3.7 V, 1'500 mAh, chargeable through microUSB port; Autonomy between 4 and 5 hours \\ \texttt{Powerbank} & 5'500 mAh, chargeable through microUSB port; Autonomy around 23 hours \\ \hline

\end{tabular}
\caption{The main characteristics of the Thymio II model}
\label{thymio_characteristics}
\end{table}
\FloatBarrier

\section{Software Architecture}

\subsection{Aseba}

Thymio can be programmed with \emph{Aseba} \citep{retornaz2013seamless}, which is a set of tools that enables to program the robot within several programming environments, namely \emph{Visual, Blockly, Text, Scratch} programming. Aseba is shipped with a command-line utility tool called \emph{asebamedulla}, that allows accessing an Aseba network through \emph{D-BUS}\footnote{\url{https://www.freedesktop.org/wiki/Software/dbus/}}. This enables to program Aseba-enabled devices, the Thymio robot, using third-party languages. Since our preferred language is Python, the python CLI\footnote{\url{https://github.com/aseba-community/aseba/blob/master/examples/clients/python-dbus/aseba.py}} was chosen, which is a thin wrapper around asebamedulla dbus interface.

D-Bus is the main IPC system used in Linux: processes expose objects with a declared interface whose methods can be called from other processes. This is implemented by sending messages over D-Bus itself. The Aseba environment provides a D-Bus interface via the asebamedulla utility, which is in charge of transmitting to the robot hardware. The abstraction is in the form of a network of Thymio robots and processes listening on D-Bus, the so-called \emph{AsebaNetwork}.

The goal of the Aseba D-Bus integration is not to be an alternative to any of the Aseba languages used to program the robots locally. Indeed, it is meant to provide integration with another system, on which not only the D-Bus deamon and asebamedulla are running, but also the programs utilizing the D-Bus interface, which are never transferred in any manner to the Thymio hardware. This, in turn, means that the thymio is continuously connected to the machine on which the Python script, in our case, is running.

The API consists ultimately in the interfaces that asebamedulla provides over D-Bus. Through this interface, we can retrieve information about the network, read and write variables, or send events.

\begin{lstlisting}[style=C-color, caption={The API that asebamedulla provides over D-Bus},label={Asebamedulla API}]

interface ch.epfl.mobots.EventFilter {
    method Void ListenEvent(UInt16 eventId)
    method Void ListenEventName(String eventName)
    method Void IgnoreEvent(UInt16 eventId)
    method Void IgnoreEventName(String eventName)
    signal Event(UInt16 id, String name, Array<SInt16> payloadData)
}

interface ch.epfl.mobots.AsebaNetwork {
    method Void LoadScripts(String fileName)
    method Array<String> GetNodesList()
    method Array<String> GetVariablesList(String nodeName)
    method Void SetVariable(String nodeName, String variableName, Array<SInt16> variableData)
    method Array<SInt16> GetVariable(String nodeName, String variableName)
    method Void SendEvent(UInt16 eventId, Array<SInt16> payloadData)
    method Void SendEventName(String eventName, Array<SInt16> payloadData)
    method ObjectPath CreateEventFilter()
}
\end{lstlisting}

The \emph{AsebaNetwork} interface allows working with all of the nodes (robots) in the same network. There are methods to retrieve a list of the connected nodes ( \emph{GetNodesList} ) and to broadcast a global event, like \emph{SendEvent}. Global events are events that aseba nodes exchange within the aseba network. On the other hand, events that are internal to the node are called \emph{local events}. \emph{SetVariable} and \emph{GetVariable}, which write and read respectively native variables of the Aseba scripting language.

Asebamedulla exposes the interface \emph{EventFilter} which allows managing events. An application that wants to listen to events have register events with \emph{ListenEventName} or \emph{ListenEvent} be notified when an event occurs. The application can receive these events through the \emph{Event} signal, these events correspond to the global events of the Aseba language.

\subsection{Robotic Simulator}

In our experiments, a set of tools is needed in order to develop, test and validate different approaches to the reality gap problem. In relation to evolutionary robotics, these tools represent robotic simulators and robotic frameworks. Robotic simulators help to build the bridge between simulation and reality, they endorse that the developed simulation can be transferred and applied to real robotics. The most well know simulators in the community are \emph{V-REP} \citep{rohmer2013v} and \emph{Gazebo} \citep{koenig2004design} based on a survey made by Ivaldi, Peters, \citep{ivaldi2014tools}. These two simulators are compared in basic robotic control logic and evolutionary robotics experiment in \citep{nogueira2014comparative}. \emph{V-REP} robot simulation framework came out as the more intuitive and user-friendly simulator, which comes with various features and is less hardware demanding. Moreover, we have experience with \emph{V-REP} from previous works, therefore it was chosen as our main robotic framework.

In this work the use of V-REP simulation is not only used for \emph{Simulation-based optimization} and \emph{Robot-in-the-loop simulation-based optimization} approaches but also for \emph{Reality-based optimization} approach. V-REP exposes a remote API that allows controlling the simulation from the external client-side application. The client-side application is written in python which enables to control the simulation over the simulation variables.

\subsection{Computer Vision}

One of the most pivotal points of the computer vision is to track the Thymio robot during evaluation and positioning of the obstacles in the simulation. Therefore it goes without saying that the system must be able to accurately read the position and angle of these objects. Since the environment is static and small in size, a fixed-camera computer vision is optimal to extract the necessary spatial information within a single image. The most necessary problem is solving the objects identification and accurate spatial information extraction. These problems are usually dealt with using certain marker systems. One of such a system is \citep{bencina2005improved}, which relies on 2D tracking of specially designed fiducials (markers) in a real-time video stream. This system involves a set of marker patterns and computer vision algorithms that can track and yield various spatial information of these markers. Such a marker system was chosen in this project as it perfectly satisfy our requirements. The choice is supported by the same computer vision system being used and proven in \citep{faina2017automating}.

\subsubsection{Fiducial tracking}

The \epmh{ReacTIVision Fiducials} \ref{fig:fiducial_markers} comes in 3 different set of sizes \emph{small, medium, large} and each individual marker has it's own unique id. Furthermore, each of them is purely identified by its unique topological structure. The system employs the topological fiducial recognition, which enables detection and identification. In this approach, a region adjacency graph is computed from a binary image. The adjacency graph can be understood as a tree. By recognizing the graphs representing the fiducials, markers can be detected and identified. Moreover, the location is computed as the weighted average of all leaf centers (black and white). The vector from this centroid to a point given by the weighted average of all black (or white) leaf centers is used to compute the orientation of the fiducial \citep{bencina2005improved}.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.66\linewidth]{include/images/fiducial_markers.PNG}
  \caption{ReacTIVision Fiducials from original paper a) a reacTIVision fiducial (b) black and white leafs and their average centroid (c) black leafs and their average centroid, and (d) the vector used to compute the orientation of the fiducial}
  \label{fig:fiducial_markers}
\end{figure}

\subsubsection{Computer Vision Implementation}

The original implementation of the \emph{reacTIVision} system has been re-implemented using \emph{OpenCV}\footnote{\url{https://opencv.org/}} by Faina, in \citep{faina2017automating}. The actual implementation is written in Python, and have been slightly modified, concretely, have been made thread-safe. The entire vision system operates on pixel-coordinates, which is turned into a base coordinate system. Transformation...

The video frame is captured by using an HD camera \footnote{Logitech 1980x1080 pixels}, which is placed on top of the physical arena \ref{fig:test_bed}. Processing a single frame consist of the following steps: 

\begin{itemize}
  \item Pre-processing. Compensate radial and tangential lens distortion.
  \item Thresholding. Transform grayscale image to a binary image.
  \item Segmentation. Construct the region adjacency graph.
  \item Fiducial Recognition. Subgraphs are identified and their location and orientation are computed.   
\end{itemize}

As was pointed out in \citep{faina2017automating}, the lighting condition has to be constant in order to be able to locate the fiducial markers. This issue arises only when sudden, lighting changes occur in the environment. For example, window blinds have been folded. Even though the vision system is able to compensate incidents such this by stabilizing the image, when the lighting conditions are too extreme (sunset/sunrise) the cameras automatic white balancing and focusing make the image to blurred to identify the fiducial markers. This issue was fixed by placing a fixture to the ceiling which ensured that the lighting condition is more or less constant.


\subsection{Simulation Restart}

One of the requirements of the evolutionary procedure applied in this projects is that the agent is required to start each iteration of the evolutionary algorithm from the same position. Naturally, for the simulation-based approach utilized in a simulation, it is not a problem, since the robot position can be easily set by the simulation tool. On the other hand, this behavior had to be implemented in the reality-based optimization approach for dealing with situations when the Thymio had to return to his initial position after an evaluation. This behavior is the \emph{path following} behavior of the Thymio vehicle. Several important facts had to be considered in the implementation. Among the most important one is the ability to track the path from any given position to the initial position. In addition to this tracking, the system has to take full control of the Thymio vehicle and operate it to follow the computed route as close as possible to the initial position, considering all errors along the way. Another important part of the path following is the consideration of the obstacles. It has to take into account the static obstacles that might be in the direction of travel.

\subsubsection{Path following}

There are many different types of path tracking algorithm available today. For example \emph{follow-the-carrot, pure pursuit} and \emph{vector pursuit}. We have chosen to implement \emph{follow-the-carrot} even though it has been proven \citep{lundgren2003path} to be the worse among the mentioned algorithms. Despite its drawbacks, the algorithm is easy to understand and the implementation was rather simple. The algorithm principle is based on a simple idea in which a point (carrot-point) on look ahead distance away from the vehicle coordinated system is used to obtain a goal-point, then aim the vehicle towards that point.

\begin{figure}[H]
  \centering	
  \includegraphics[width=6cm]{include/images/follow_the_carrot.PNG}
  \caption{Follow-the-carrot basic principles.}
  \label{fig:follow_the_carrot}
\end{figure}

Figure \ref{fig:follow_the_carrot} describes the basic principles. The most important parameter is the orientation error, describing the current angle of the robot and the line drawn from the center of the robot to the carrot point. A simple proportional controller is used to minimize the orientation error between the vehicle and the carrot point. Which then controls the robots linear velocity and rotational velocity to reach the carrot point. The entire path following system is depicting in the following figure \ref{fig:path_following_diagram}. The camera vision system is used to obtain the position of all the four corners, including the position of the obstacles. Next, a 2-dimensional grid is created, representing the physical arena as a graph, while taking into account the walls and obstacles. The robots current position and orientation are obtained and the goal (initial) position likewise. To find the path between the robot and goal (initial) position we use a graph search algorithm, namely A* algorithm \footnote{\url{https://en.wikipedia.org/wiki/A*_search_algorithm}}. The algorithm takes the 2-dimensional grid as input and outputs the carrot points that the robot has to follow. Then the control loop is responsible to guide the robot based on the follow-the-carrot algorithm until the goal (initial) position is reached.

\begin{figure}[H]
	\centering
  	\includegraphics[width=6cm]{include/images/dummy.PNG}
  	\caption{Path following system diagram.}
  	\label{fig:path_following_diagram}
\end{figure}

\section{Automation Control}

\subsection{Evaluation of the robotic platform}
